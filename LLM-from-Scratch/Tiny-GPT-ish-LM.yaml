# ===================== Tiny GPT-ish LM trained on two words =====================
# Inspired by GPT-2 config files released by OpenAI.
# Trainable in <30â€¯s on CPU; produces ~26â€¯k parameters total.
# -------------------------------------------------------------------------------
#  Corpus: the twoâ€‘word vocabulary {"apple", "banana"}.
#  Special tokens: <bos>, <eos>, <pad>. (We do **not** use an <unk> token because
#  every corpus token is inâ€‘vocab.)
# -------------------------------------------------------------------------------

model_type: gpt2
vocab_size: 4               # <bos>, <eos>, apple, <pad>
bos_token_id: 0
eos_token_id: 1
pad_token_id: 3

# ---- tinyâ€‘model geometry -------------------------------------------------------
n_positions: 16             # maximum sequence length (enough for <bos> word <eos>)
n_embd: 32                  # embedding / hidden size
n_layer: 2                  # Transformer decoder layers
n_head: 4                   # must divide n_embd evenly (32 Ã· 4 = 8â€‘d head)
n_inner: 128                # feedâ€‘forward dimension (4 Ã— n_embd, GPTâ€‘2 default)

# ---- regularisation zeroed so the toy model converges fast --------------------
resid_pdrop: 0.0
attn_pdrop: 0.0
embd_pdrop: 0.0
layer_norm_eps: 1.0e-5
initializer_range: 0.02

# ---- training hyperâ€‘parameters -------------------------------------------------
trainer:
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 1
  learning_rate: 5.0e-4      # high LR is fine for <30â€¯k params
  weight_decay: 0.0
  lr_scheduler_type: cosine
  num_train_epochs: 500       # ~10â€¯k updates; stops as soon as lossâ‰ˆ0
  warmup_steps: 10
  logging_steps: 50
  save_steps: 200
  max_steps: -1              # unlimited; use epochs instead

# ---- tokenizer spec (Hugging Face WordLevel) ----------------------------------
tokenizer:
  type: wordlevel
  vocab:
    "<bos>": 0
    "<eos>": 1
    "apple": 2
    "<pad>": 3
  special_tokens: ["<bos>", "<eos>", "<pad>"]

# ---- dataset (inline for brevity) ---------------------------------------------
dataset:
  train:
    sequences:
      - "apple"
      - "banana"
  val:
    sequences:
      - "banana"

# Each sequence is wrapped as: <bos> WORD <eos> (â†’ 3 tokens).
# -------------------------------------------------------------------------------
#  PARAMETER COUNT (dense):
#   â€¢ Token + position embeddings  = 640
#   â€¢ Transformer layers (Ã—2)      = 25â€¯408
#   â€¢ LM head (tied to embeddings) =   128
#   --------------------------------------------------
#                            TOTAL â‰ˆÂ 26â€¯176 params
# -------------------------------------------------------------------------------
#  TRAINING COMMAND (ðŸ¤—Â Transformers â‰¥Â 4.41):
#    accelerate launch train.py \
#      --config_file tiny_gpt_config.yaml \
#      --output_dir ./tiny-gpt-two-words
# -------------------------------------------------------------------------------
# After convergence, the model will deterministically map either input word to
# itself with perplexityÂ â‰ˆÂ 1.0 and generate infinite repetitions of the same
# word when sampled.
# ==============================================================================
